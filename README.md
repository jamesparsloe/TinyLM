# TinyLM

My playground for language modeling.

## Data

I'll by using the TinyStories dataset from [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?](https://arxiv.org/abs/2305.07759).

## Models

- [x] GPT
- [ ] Mamba
- [ ] Based
- [ ] ...

## Experiments

- <https://wandb.ai/jamesparsloe/TinyLM>

### Just training the `LayerNorm`s

- Inspired by [Training BatchNorm and Only BatchNorm: On the Expressive Power of Random Features in CNNs](https://arxiv.org/abs/2003.00152) (Frankle et al., 2020)

- <https://wandb.ai/jamesparsloe/TinyLM/runs/tury2cp5>

- `./runs/tury2cp5/TinyLM-006000.pt`
